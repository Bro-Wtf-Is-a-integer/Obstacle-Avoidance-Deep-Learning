{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# License Notice\n",
        "# Copyright (c) 2025 Duc Huy Vu, Hieu Minh Tran\n",
        "#\n",
        "# This code is for personal and academic use only.\n",
        "# Only the authors may modify, distribute, or reuse it.\n",
        "# Viewing for grading purposes is allowed.\n",
        "# -----------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "v_ggMJnb-8vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in model.py (or wherever)\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "YXvT4TTrhs_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell 1 — Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG6xIjw6ittN",
        "outputId": "fffca8fc-003e-48f1-df4b-b9b05ac00851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Model Setup\n"
      ],
      "metadata": {
        "id": "sAGMg21dqm1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as nn_utils\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "from typing import Tuple\n",
        "\n",
        "class DepthwiseSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 kernel_size: int = 3, padding: int = 1,\n",
        "                 num_groups: int = 8):\n",
        "        super().__init__()\n",
        "        # depthwise\n",
        "        self.depthwise = nn.Conv2d(\n",
        "            in_channels, in_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            groups=in_channels,\n",
        "            bias=False\n",
        "        )\n",
        "        # pointwise\n",
        "        self.pointwise = nn.Conv2d(\n",
        "            in_channels, out_channels,\n",
        "            kernel_size=1,\n",
        "            bias=False\n",
        "        )\n",
        "        # use GroupNorm for small‐batch stability\n",
        "        self.gn = nn.GroupNorm(num_groups=min(num_groups, out_channels),\n",
        "                               num_channels=out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        x = self.gn(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "class EfficientCNNEncoder(nn.Module):\n",
        "    def __init__(self, in_channels: int = 1, out_channels: int = 8):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            DepthwiseSeparableConv(in_channels, 16),\n",
        "            DepthwiseSeparableConv(16,          out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.encoder(x)"
      ],
      "metadata": {
        "id": "1gucw_QSqlgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weights = self.attn(x)         # (B, T, 1)\n",
        "        weights = self.softmax(weights)\n",
        "        context = torch.sum(weights * x, dim=1)  # (B, hidden_dim)\n",
        "        return context"
      ],
      "metadata": {
        "id": "D1yhscqGhzSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientUAVNavigationModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int            = 9,\n",
        "        hidden_dim:   int            = 128,\n",
        "        encoder_out:  int            = 8,\n",
        "        image_size:   Tuple[int,int] = (256, 256),\n",
        "        debug:        bool           = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.debug = debug\n",
        "\n",
        "        # -- spatial encoder --\n",
        "        self.encoder = EfficientCNNEncoder(in_channels=1,\n",
        "                                           out_channels=encoder_out)\n",
        "        self.cnn = nn.Sequential(\n",
        "            DepthwiseSeparableConv(encoder_out, 64),\n",
        "            nn.MaxPool2d(2),\n",
        "            DepthwiseSeparableConv(64,         128),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        # -- check downsampling is ≥2×2 --\n",
        "        # run a dummy tensor through to verify\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, encoder_out, *image_size)\n",
        "            feat  = self.cnn(dummy)\n",
        "            Hf, Wf = feat.shape[-2], feat.shape[-1]\n",
        "            assert Hf >= 2 and Wf >= 2, \\\n",
        "                f\"Feature map {Hf}×{Wf} too small for 2×2 SPP\"\n",
        "\n",
        "        # -- spatial pyramid pooling into 2×2 bins --\n",
        "        self.spp = nn.AdaptiveAvgPool2d((2, 2))  # outputs (B*T, 128, 2, 2)\n",
        "\n",
        "        # -- projection down to GRU input dim --\n",
        "        self.proj    = nn.Linear(128 * 4, 128)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        # -- temporal model --\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=128,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # -- attention & final classifier --\n",
        "        self.attn    = nn.Linear(hidden_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.fc      = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        # initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.GRU):\n",
        "                # leave GRU defaults or add custom init here if desired\n",
        "                pass\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, 1, H, W)\n",
        "        returns logits: (B, num_classes)\n",
        "        \"\"\"\n",
        "        B, T, C, H, W = x.shape\n",
        "        # merge batch & time dims\n",
        "        x = x.view(B * T, C, H, W)           # → (B*T, 1, H, W)\n",
        "\n",
        "        # spatial features\n",
        "        x = self.encoder(x)                  # → (B*T, encoder_out, H, W)\n",
        "        x = self.cnn(x)                      # → (B*T, 128, H', W')\n",
        "\n",
        "        # 2×2 spatial pyramid pooling\n",
        "        x = self.spp(x)                      # → (B*T, 128, 2, 2)\n",
        "        x = x.view(B, T, 128 * 4)            # → (B, T, 512)\n",
        "\n",
        "        # project & dropout\n",
        "        x = self.proj(x)                     # → (B, T, 128)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # temporal modeling\n",
        "        x, _ = self.gru(x)                   # → (B, T, hidden_dim)\n",
        "\n",
        "        # attention pooling\n",
        "        weights = self.attn(x)               # → (B, T, 1)\n",
        "        weights = self.softmax(weights)      # → (B, T, 1)\n",
        "        context = torch.sum(weights * x, dim=1)  # → (B, hidden_dim)\n",
        "\n",
        "        # classification\n",
        "        logits = self.fc(context)            # → (B, num_classes)\n",
        "\n",
        "        if self.debug:\n",
        "            print(f\"[forward] logits.shape = {logits.shape}\")\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "rkfRgZETEF2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class DepthDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.files = sorted(glob.glob(os.path.join(data_dir, \"*.npy\")))\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        arr = np.load(self.files[idx]).astype(np.float32)  # [H,W]\n",
        "        if self.transform:\n",
        "            return self.transform(arr), 0\n",
        "        else:\n",
        "            return torch.from_numpy(arr).unsqueeze(0), 0\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # adds channel dim\n",
        "    # transforms.Normalize(mean=[…], std=[…])  # if you used it\n",
        "])\n"
      ],
      "metadata": {
        "id": "-EgNcVj5gN9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ncyHr6WLCe_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running"
      ],
      "metadata": {
        "id": "C0rOVvGPVbei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1) Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2) Create & move model\n",
        "model = EfficientUAVNavigationModel(\n",
        "    num_classes=9,\n",
        "    hidden_dim=128,\n",
        "    encoder_out=8,\n",
        "    image_size=(256,256),\n",
        "    debug=False\n",
        ").to(device)\n",
        "\n",
        "# 3) Load weights\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/Colab Notebooks/FastyUpgrades.pt\",\n",
        "                        map_location=device)\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# 4) Eval mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "PaLWoJcagS4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Frames FIFO"
      ],
      "metadata": {
        "id": "LRzQ3K8kXgCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from IPython.display import display, Image  # 引入 IPython.display 用於顯示圖片\n",
        "\n",
        "# --- 0) Configuration ---\n",
        "frame_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/Data_png\"\n",
        "seq_len   = 10\n",
        "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pdf_output_path = \"/content/drive/MyDrive/Colab Notebooks/output.pdf\"  # Output PDF path\n",
        "\n",
        "label_map = {\n",
        "    0: 'forward', 1: 'backward', 2: 'up', 3: 'down',\n",
        "    4: 'left', 5: 'right', 6: 'stop', 7: 'rotate_left', 8: 'rotate_right'\n",
        "}\n",
        "\n",
        "# --- 1) Inference helper ---\n",
        "def infer_and_time(model, input_tensor, device):\n",
        "    model.eval()\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    pred    = logits.argmax(dim=1).item()\n",
        "    elapsed = time.time() - start\n",
        "    return pred, elapsed\n",
        "\n",
        "# --- 2) Load & preprocess frames ---\n",
        "frame_paths = sorted(glob.glob(f\"{frame_dir}/*.png\"))\n",
        "frames = []\n",
        "\n",
        "for p in frame_paths:\n",
        "    img     = cv2.imread(p, cv2.IMREAD_COLOR)\n",
        "    gray    = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "    normed  = resized.astype(np.float32) / 255.0\n",
        "    tensor  = torch.from_numpy(normed).unsqueeze(0)  # shape [1,256,256]\n",
        "    frames.append(tensor)\n",
        "\n",
        "assert len(frames) >= seq_len+1, f\"Need at least {seq_len+1} frames, got {len(frames)}\"\n",
        "\n",
        "# --- 3) Pre-fill a FIFO buffer of the first `seq_len` frames ---\n",
        "buffer = deque(maxlen=seq_len)\n",
        "for i in range(seq_len):\n",
        "    buffer.append(frames[i])   # each element is [1,256,256] tensor\n",
        "\n",
        "# --- 4) Create PDF and process frames ---\n",
        "with PdfPages(pdf_output_path) as pdf:\n",
        "    for idx in range(seq_len, len(frames)):\n",
        "        new_frame = frames[idx]\n",
        "        buffer.append(new_frame)  # drop oldest, add newest\n",
        "\n",
        "        # Build input sequence tensor of shape [1, T=seq_len, C=1, H=256, W=256]\n",
        "        seq_tensor = torch.stack(list(buffer), dim=0)      # [T,1,H,W]\n",
        "        seq_tensor = seq_tensor.unsqueeze(0)               # [1,T,1,H,W]\n",
        "\n",
        "        # Run inference & measure latency\n",
        "        pred, latency = infer_and_time(model, seq_tensor, device)\n",
        "\n",
        "        # Display result in console\n",
        "        start_frame = idx - seq_len + 1\n",
        "        end_frame   = idx\n",
        "        print(f\"Buffer frames {start_frame}–{end_frame} → \"\n",
        "              f\"Pred for frame {idx+1}: {label_map[pred]} \"\n",
        "              f\"({latency*1000:.1f} ms)\")\n",
        "\n",
        "        # Create the figure for both displaying and saving to PDF\n",
        "        img = new_frame.squeeze().cpu().numpy()\n",
        "        plt.figure(figsize=(4,4))\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Frame {idx+1} → Pred: {label_map[pred]}\", fontsize=14)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Save the figure to a temporary buffer for displaying in Colab\n",
        "        from io import BytesIO\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "        buf.seek(0)\n",
        "\n",
        "        # Display the figure in Colab\n",
        "        display(Image(buf.getvalue()))\n",
        "\n",
        "        # Save the figure to the PDF\n",
        "        pdf.savefig()  # Save the current figure to the PDF\n",
        "        plt.close()    # Close the figure to free memory\n",
        "\n",
        "print(f\"PDF saved to {pdf_output_path}\")"
      ],
      "metadata": {
        "id": "rVoCc8kBYP0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save_data"
      ],
      "metadata": {
        "id": "w31J2qwjM8d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# --- 0) Configuration ---\n",
        "frame_dir = \"/content/drive/MyDrive/Colab Notebooks/Data/Data_png\"\n",
        "seq_len = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pdf_output_path = \"/content/drive/MyDrive/Colab Notebooks/angelo_output.pdf\"  # Output PDF path\n",
        "\n",
        "label_map = {\n",
        "    0: 'forward', 1: 'backward', 2: 'up', 3: 'down',\n",
        "    4: 'left', 5: 'right', 6: 'stop', 7: 'rotate_left', 8: 'rotate_right'\n",
        "}\n",
        "\n",
        "# --- 1) Inference helper ---\n",
        "def infer_and_time(model, input_tensor, device):\n",
        "    model.eval()\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    pred = logits.argmax(dim=1).item()\n",
        "    elapsed = time.time() - start\n",
        "    return pred, elapsed\n",
        "\n",
        "# --- 2) Load & preprocess frames ---\n",
        "frame_paths = sorted(glob.glob(f\"{frame_dir}/*.png\"))\n",
        "frames = []\n",
        "\n",
        "for p in frame_paths:\n",
        "    img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (256, 256), interpolation=cv2.INTER_AREA)\n",
        "    normed = resized.astype(np.float32) / 255.0\n",
        "    tensor = torch.from_numpy(normed).unsqueeze(0)  # shape [1,256,256]\n",
        "    frames.append(tensor)\n",
        "\n",
        "assert len(frames) >= seq_len + 1, f\"Need at least {seq_len + 1} frames, got {len(frames)}\"\n",
        "\n",
        "# --- 3) Pre-fill a FIFO buffer of the first `seq_len` frames ---\n",
        "buffer = deque(maxlen=seq_len)\n",
        "for i in range(seq_len):\n",
        "    buffer.append(frames[i])  # each element is [1,256,256] tensor\n",
        "\n",
        "# --- 4) Create PDF and process frames ---\n",
        "with PdfPages(pdf_output_path) as pdf:\n",
        "    for idx in range(seq_len, len(frames)):\n",
        "        new_frame = frames[idx]\n",
        "        buffer.append(new_frame)  # drop oldest, add newest\n",
        "\n",
        "        # Build input sequence tensor of shape [1, T=seq_len, C=1, H=256, W=256]\n",
        "        seq_tensor = torch.stack(list(buffer), dim=0)  # [T,1,H,W]\n",
        "        seq_tensor = seq_tensor.unsqueeze(0)  # [1,T,1,H,W]\n",
        "\n",
        "        # Run inference & measure latency\n",
        "        pred, latency = infer_and_time(model, seq_tensor, device)\n",
        "\n",
        "        # Display result in console\n",
        "        start_frame = idx - seq_len + 1\n",
        "        end_frame = idx\n",
        "        print(f\"Buffer frames {start_frame}–{end_frame} → \"\n",
        "              f\"Pred for frame {idx+1}: {label_map[pred]} \"\n",
        "              f\"({latency*1000:.1f} ms)\")\n",
        "\n",
        "        # Create the figure for both displaying and saving to PDF\n",
        "        img = new_frame.squeeze().cpu().numpy()\n",
        "        plt.figure(figsize=(3, 3), dpi=300)  # Smaller size for paper, high DPI\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"Frame {idx+1}: {label_map[pred]}\", fontsize=10)  # Smaller font\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout(pad=0.5)  # Minimize padding\n",
        "\n",
        "        # Save the figure to a temporary buffer for displaying in Colab\n",
        "        from io import BytesIO\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0.05, dpi=300)\n",
        "        buf.seek(0)\n",
        "\n",
        "        # Display the figure in Colab\n",
        "        display(Image(buf.getvalue()))\n",
        "\n",
        "        # Save the figure to the PDF\n",
        "        pdf.savefig(bbox_inches='tight', pad_inches=0.05, dpi=300)  # High-quality PDF\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "print(f\"PDF saved to {pdf_output_path}\")"
      ],
      "metadata": {
        "id": "Cvh9iI6-NE9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}